---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```
# textrecipes

<!-- badges: start -->
[![R build status](https://github.com/tidymodels/textrecipes/workflows/R-CMD-check/badge.svg)](https://github.com/tidymodels/textrecipes/actions)
[![Codecov test coverage](https://codecov.io/gh/tidymodels/textrecipes/branch/master/graph/badge.svg)](https://codecov.io/github/tidymodels/textrecipes?branch=master)
[![CRAN status](http://www.r-pkg.org/badges/version/textrecipes)](https://CRAN.R-project.org/package=textrecipes)
[![Downloads](http://cranlogs.r-pkg.org/badges/textrecipes)](https://CRAN.R-project.org/package=textrecipes)
[![Lifecycle: maturing](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
<!-- badges: end -->

## Introduction

**textrecipes** contain extra steps for the [`recipes`](https://CRAN.R-project.org/package=recipes) package for preprocessing text data. 

## Installation

You can install the released version of textrecipes from [CRAN](https://CRAN.R-project.org) with:

```{r, eval=FALSE}
install.packages("textrecipes")
```

Install the development version from GitHub with:

```{r installation, eval=FALSE}
require("devtools")
install_github("tidymodels/textrecipes")
```

## Example

In the following example we will go through the steps needed, to convert a character variable to the TF-IDF of its tokenized words after removing stopwords, and, limiting ourself to only the 100 most used words. The preprocessing will be conducted on the variable `essay0` and `essay1`.

```{r, message=FALSE}
library(recipes)
library(textrecipes)

data(okc_text)

okc_rec <- recipe(~ ., data = okc_text) %>%
  step_tokenize(essay0, essay1) %>% # Tokenizes to words by default
  step_stopwords(essay0, essay1) %>% # Uses the english snowball list by default
  step_tokenfilter(essay0, essay1, max_tokens = 100) %>%
  step_tfidf(essay0, essay1)
   
okc_obj <- okc_rec %>%
  prep(training = okc_text)
   
str(bake(okc_obj, okc_text), list.len = 15)
```


## Type chart

**textrecipes** includes a little departure in design from **recipes**, in the sense that it allows for some input and output to be in the form of list columns. To avoid confusion, here is a table of steps with their expected input and output respectively. Notice how you need to end with numeric for future analysis to work.

| Step                       | Input       | Output      |
|----------------------------|-------------|-------------|
| `step_tokenize()`          | character   | list-column |
| `step_untokenize()`        | list-column | character   |
| `step_lemma()`             | list-column | list-column |
| `step_stem()`              | list-column | list-column |
| `step_stopwords()`         | list-column | list-column |
| `step_tokenfilter()`       | list-column | list-column |
| `step_tokenmerge()`        | list-column | list-column |
| `step_tfidf()`             | list-column | numeric     |
| `step_tf()`                | list-column | numeric     |
| `step_texthash()`          | list-column | numeric     |
| `step_word_embeddings()`   | list-column | numeric     |
| `step_textfeature()`       | character   | numeric     |
| `step_sequence_onehot()`   | character   | numeric     |
| `step_lda()`               | character   | numeric     |

This means that valid sequences includes

```{r, eval=FALSE}
recipe(~ ., data = data) %>%
  step_tokenize(text) %>%
  step_stem(text) %>%
  step_stopwords(text) %>%
  step_topwords(text) %>%
  step_tf(text)

# or

recipe(~ ., data = data) %>%
  step_tokenize(text) %>%
  step_stem(text) %>%
  step_tfidf(text)
```

